---
title: glusterFs 3 중화 구
published: true
tags:
  - glusterfs
  - lvm
  - autofs
  - pacemaker
---
#### 환경 
- centos7.6 
- gluster 3중화
	- 호스트들은 Pacemaker로 통한 VIP 할당되어 있는상태
	- 호스트들 물리디스크 추가
	- 설치는 root
- 클라이언트는 autofs설치되어 있는 상태
	- 파일 쓰기는 일반 유저


---

#### 호스트마다 name 설정

참고 : 기존 것 `xxx_primary` 이런 (_) 특수문자 hostname은 glusterfs에서 허용하지 않음.
모든 호스트 및 사용할 클라이언트는 hostname을 알고 있어야한다. 이유 :`트러블슈팅 1`
```bash
> sudo vi /etc/hosts
xxx.xxx.xxx.xx4 xxx_primary   mdbprimary
xxx.xxx.xxx.xx5 xxx_standby01 mdbstandby01
xxx.xxx.xxx.xx6 xxx_standby02 mdbstandby02
```

---
#### 호스트 마다 방화벽 open
`주의` mount 를 하기위해서는 기본 24007 말고도 8,9 .. 등등 다른 포트도 필요하다.
```bash
> firewall-cmd --add-service=glusterfs --permanent
> sudo firewall-cmd --reload
```

---
#### 호스트 1에서 다른 호스트끼리 ssh 연결 설정
peer 설정을 하기 위함.
```bash
> ssh-keygen
> ssh-copy-id xxx_standby01
> ssh-copy-id xxx_standby02

# 접속확인
> ssh xxx_standby01          
> ssh xxx_standby02          
```

---
#### 호스트 마다 repo update
```bash
> sudo yum update
```

---
#### 호스트 마다 glusterfs 설치
```bash
> sudo yum install centos-release-gluster
> sudo yum install  glusterfs-server
> sudo yum install  glusterfs 
> systemctl enable glusterd
> systemctl start glusterd

```

---
#### 호스트1 에서 peer 설정
```bash
# 없음을 확인
> sudo gluster peer status       
Number of Peers: 0

# 다른 호스트 연결
> sudo gluster peer probe "MDBstandby01" 
> sudo gluster peer probe "MDBstandby02" 

# 잘못 등록 될 시 해제
> sudo gluster peer detach "MDBstandby01"

# 연결됨 확인 (ip address가 아닌 name 형식으로 등록하길 추천한다.)
> sudo gluster peer status            
Number of Peers: 2                                                       
                                                                         
Hostname: MDBstandby01                                                   
Uuid: 85e5b596-5d91-49d8-a5aa-10f1304a5364                               
State: Peer in Cluster (Connected)                                       
                                                                         
Hostname: MDBstandby02                                                   
Uuid: 440e17f7-6b4f-4e85-841a-3acbd199d8a3                               
State: Peer in Cluster (Connected)                                       
```

---
#### 호스트1 마다 공유디렉토리 생성
```bash
> cd ...
> mkdir .../web-share-file
```

---
#### 호스트1 에서 공유 디렉토리 볼륨생성1

root의 하위 디렉토리에 gluster 볼륨 생성 시 문제생길 수 있으므로.. 추천하지 않음.
예를 들어 다음과 같은 문제 발생할 수 있음.

1.디스크 풀로 인한 기존 볼륨 이미 존재하지만 찾기 실패
2.볼륨 재 생성 시 이미 존재한 볼륨이 존재..
3.반복

```bash
> sudo gluster volume create storage replica 3 arbiter 1 mdbprimary:.../web-share-file  MDBstandby01:.../web-share-file  MDBstandby02:.../web-share-file  

volume create: storage: failed: The brick mdbprimary:.../web-share-file is a mount point. Please create a sub-directory under the mount point and us
e that as the brick directory. Or use 'force' at the end of the command if you want to override this behavior.                                                 
```

##### 물리 디스크 생성 후 lvm 생성
```bash
> lsblk                                                               
> sudo pvcreate /dev/sdb                                              
> sudo vgcreate sharedisk /dev/sdb                                    
> sudo lvcreate -L 10G -n root sharedisk                              
> mkfs.xfs /dev/mapper/sharedisk-root                                 
> sudo mkfs.xfs /dev/mapper/sharedisk-root                            
```

##### lvm 마운트 해놓기
```bash
> sudo mount /dev/mapper/sharedisk-root .../web-share-file
> mount                                                               
```

##### 호스트 재시작 시 자동 lvm 마운트 설정
```bash
> sudo vi /etc/fstab
/dev/mapper/sharedisk-root               .../web-share-file   xfs  defaults        0 0
```

---
#### 호스트1 에서 공유 디렉토리 볼륨생성2

lv 마운트 포인터와 gluster 볼륨 포인트가 있다면 앞의 문제로 순서가 맞지 않을 시 데이터의 문제가 발생될 가능성 있다고 시사하여 하위에 brick이라는 디렉토리를 만들고 볼륨을 생성하라고 추천함.

```bash
Imagine you have a LV you want to use for the gluster volume. Now you mount this LV to /mnt/gluster1. You do this on the other host(s), too and you create the gluster volume with /mnt/gluster1 as brick. By mistake you forget to add the mount entry to fstab so the next time you reboot server1, /mnt/gluster1 will be there (because it's the mountpoint) but the data is gone (because the LV is not mounted). I don't know how gluster would handle that but it's actually easy to try it out :)
So using a subfolder within the mountpoint makes sense, because that subfolder will not exist when the mount of the LV didn't happen.
```

---
#### 각 호스트마다 brick 폴더 생성
```bash
> sudo mkdir .../web-share-file/brick

# 디렉토리에 권한 할당필요 시
> sudo chown -R ..user: .../web-share-file
```


---
#### 호스트1 에서 공유 디렉토리 볼륨생성3
```bash
> sudo gluster volume create storage replica 3 arbiter 1 mdbprimary:.../web-share-file/brick  MDBstandby01:.../web-share-file/brick  MDBstandby02:.../web-share-file/brick                                     

volume create: storage: success: please start the volume to access data     
```

---
#### 호스트1 에서 볼륨 시작
```bash
> sudo gluster volume start storage  
volume start: storage: success                                           
```

---
#### 호스트1 에서 확인
```bash
> sudo gluster vol info     
Volume Name: storage                                                          
Type: Replicate                                                               
Volume ID: 7400b61c-fd38-4507-b2d5-fa4adf2fb44c                               
Status: Started                                                               
Snapshot Count: 0                                                             
Number of Bricks: 1 x (2 + 1) = 3                                             
Transport-type: tcp                                                           
Bricks:                                                                       
Brick1: mdbprimary:.../web-share-file/brick                       
Brick2: MDBstandby01:.../web-share-file/brick                     
Brick3: MDBstandby02:.../web-share-file/brick (arbiter)           
Options Reconfigured:                                                         
cluster.granular-entry-heal: on                                               
storage.fips-mode-rchecksum: on                                               
transport.address-family: inet                                                
nfs.disable: on                                                               
performance.client-io-threads: off                                            

> sudo gluster volume status               
Status of volume: storage                                                     
Gluster process                             TCP Port  RDMA Port  Online  Pid  
------------------------------------------------------------------------------
Brick mdbprimary:.../web-share-                                   
file/brick                                  49152     0          Y       31069
Brick MDBstandby01:.../web-shar                                   
e-file/brick                                49152     0          Y       27948
Brick MDBstandby02:.../web-shar                                   
e-file/brick                                49152     0          Y       31334
Self-heal Daemon on localhost               N/A       N/A        Y       31086
Self-heal Daemon on MDBstandby02            N/A       N/A        Y       31351
Self-heal Daemon on MDBstandby01            N/A       N/A        Y       27965
                                                                              
Task Status of Volume storage                                                 
------------------------------------------------------------------------------
There are no active volume tasks                                              
```


---
#### 호스트 2 에서 확인
```bash
> sudo gluster vol info                                                         
Volume Name: storage
Type: Replicate
Volume ID: 7400b61c-fd38-4507-b2d5-fa4adf2fb44c
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x (2 + 1) = 3
Transport-type: tcp
Bricks:
Brick1: mdbprimary:.../web-share-file/brick
Brick2: MDBstandby01:.../web-share-file/brick
Brick3: MDBstandby02:.../web-share-file/brick (arbiter)
Options Reconfigured:
cluster.granular-entry-heal: on
storage.fips-mode-rchecksum: on
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off

> sudo gluster volume status                               
Status of volume: storage
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick mdbprimary:.../web-share-
file/brick                                  49152     0          Y       31069
Brick MDBstandby01:.../web-shar
e-file/brick                                49152     0          Y       27948
Brick MDBstandby02:.../web-shar
e-file/brick                                49152     0          Y       31334
Self-heal Daemon on localhost               N/A       N/A        Y       27965
Self-heal Daemon on xxx_primary         N/A       N/A        Y       31086
Self-heal Daemon on MDBstandby02            N/A       N/A        Y       31351
 
Task Status of Volume storage
------------------------------------------------------------------------------
There are no active volume tasks
```

---
#### 호스트 3 에서 확인
```bash
> sudo  gluster vol info                                         
Volume Name: storage
Type: Replicate
Volume ID: 7400b61c-fd38-4507-b2d5-fa4adf2fb44c
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x (2 + 1) = 3
Transport-type: tcp
Bricks:
Brick1: mdbprimary:.../web-share-file/brick
Brick2: MDBstandby01:.../web-share-file/brick
Brick3: MDBstandby02:.../web-share-file/brick (arbiter)
Options Reconfigured:
cluster.granular-entry-heal: on
storage.fips-mode-rchecksum: on
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off

> sudo gluster volume status
Status of volume: storage
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick mdbprimary:.../web-share-
file/brick                                  49152     0          Y       31069
Brick MDBstandby01:.../web-shar
e-file/brick                                49152     0          Y       27948
Brick MDBstandby02:.../web-shar
e-file/brick                                49152     0          Y       31334
Self-heal Daemon on localhost               N/A       N/A        Y       31351
Self-heal Daemon on MDBstandby01            N/A       N/A        Y       27965
Self-heal Daemon on xxx_primary         N/A       N/A        Y       31086
 
Task Status of Volume storage
------------------------------------------------------------------------------
There are no active volume tasks
```


---
#### [관리] 호스트1 에서 볼륨 삭제
```bash
> sudo gluster volume stop storage
> sudo gluster volume delete storage

# 이미 .../web-share-file/brick에 .glusterfs 가 존재하기 때문
# 각노드마드 .glusterfs 삭제 혹은 생성 시 force를 한다. (삭제를 추천.)

# 1번째 방법 : create 시 강제 force
> sudo gluster volume create storage replica 3 arbiter 1 mdbprimary:.../web-share-file/brick  MDBstandby01:/etc/c
lovir-vdi/web-share-file/brick  MDBstandby02:.../web-share-file/brick                                                                               
volume create: storage: failed: .../web-share-file/brick is already part of a volume                                                               

> sudo gluster volume create storage replica 3 arbiter 1 mdbprimary:.../web-share-file/brick  MDBstandby01:.../web-share-file/brick  MDBstandby02:.../web-share-file/brick force 

volume create: storage: success: please start the volume to access data  

# 2번째 방법 : 각 노드마다 삭제
> sudo setfattr -x trusted.glusterfs.volume-id .../web-share-file/brick  
> sudo setfattr -x trusted.gfid .../web-share-file/brick   
> sudo rm -rf .../web-share-file/brick/.glusterfs       
```


---
#### 호스트1 에서 네이티브 마운트 테스트
마운트 방식중 여러 방식이 있는데, 여기서는 네이티브 방식으로 한다.
추가로 cifs, nfs 마운트 방법도 있다.
```bash
> mkdir .../doc
> sudo mount -t  glusterfs mdbstandby02:/storage ./doc
> mount 
...
mdbstandby02:/storage on .../doc type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)
```


#### 호스트1 에서 파일쓰기 테스트
```bash

# 1. 호스트1의 brick 디렉토리에 직접 데이터 쓴경우
#    다른 호스트 2, 3에는 해당 파일이 보이지 않는다.
#		호스트에 brick으로 마운트되어 있을때 쓰기 시 반영이됨.

# 2. 호스트1의 물리 디렉토리 위치에 권한을 유저권한 변경. 
#    다른 호스트 2,3의 권한은 변경되지 않음.
#		마운트되어 있는 곳에서 파일 쓰기 할 시 '허가 거부' 발생

# 3. 모든 호스트의 물리 디렉토리 위치에 권한을 유저권한 변경.
#		마운트되어 있는 곳에서 파일 쓰기 할 시 반영됨.
```


#### 클라이언트들에서 클라이언트 설치
```bash
> sudo yum install glusterfs-fuse
```


---
#### 클라이언트들에서 설정 마무리
```bash
> mkdir .../doc

> sudo vi /etc/auto.direct
.../doc -fstype=glusterfs VIP:/storage

> sudo systemctl restart autofs
```

#### [트러블슈팅 1] 클라이언트 1에서 마운트 테스트
```bash
> sudo mount -t glusterfs VIP:/storage ./doc
Mount failed. Check the log file  for more details.

# 클라이언트 마운트 로그 위치
> sudo vi sudo tail -333f /var/log/glusterfs/...-...-xxx.log 
...
[2022-01-26 03:37:52.093155] E [MSGID: 101075] [common-utils.c:508:gf_resolve_ip6] 0-resolver: getaddrinfo failed (family:2) (이름 혹은 서비스를 알 수 없습니다)
...

# 클라이언트에서 /etc/hosts에 호스트의 명이 제대로 등록안되어 있는 문제.

```

----
#### [관리] 호스트들에서 추가 공유할 디렉토리 ( brick )
```bash
# 앞서 디렉토리 권한을 일반 유저로 변경했기때문에 sudo 안함.
> mkdir .../web-share-file/web-doc-file
```
#### 호스트1 에서 볼륨생성
```bash
> sudo gluster volume create storageDoc replica 3 arbiter 1 mdbprimary:.../web-share-file/web-doc-file  MDBstandby01:.../web-share-file/web-doc-file  MDBstandby02:.../web-doc-file                                               
volume create: storageDoc: success: please start the volume to access data    

> sudo gluster volume start storageDoc                                    
volume start: storageDoc: success                                          
```

#### 클라이언트들에서 추가 설정 마무리
```bash
> mkdir .../upload

> sudo vi /etc/auto.direct 
...
.../upload -fstype=glusterfs VIP:/storageDoc

> sudo systemctl restart autofs
```
